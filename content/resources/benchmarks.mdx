# Benchmarks
AI benchmarks are standardized datasets, tests, or evaluation methods used to measure the performance of various AI systems.

| Benchmark                                                           | Description                                                                                                                     |
| ------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| [MMLU](https://arxiv.org/abs/2009.03300)                            | Tests a model's ability to perform well on a wide range of tasks across 57 different domains like math, history, law, and more. |
| [HellaSwag](https://arxiv.org/abs/1905.07830)                       | Challenges LLMs to demonstrate commonsense reasoning and inference abilities.                                                   |
| [PIQA](https://arxiv.org/abs/1911.11641)                            | Evaluates a model's ability to answer science questions grounded in physical intuition and world knowledge.                     |
| [SIQA](https://arxiv.org/abs/1904.09728)                            | Assesses an LLM's commonsense reasoning and understanding of social situations.                                                 |
| [BoolQ](https://arxiv.org/abs/1905.10044)                           | Measures models on yes/no questions that often require complex reasoning.                                                       |
| [Winogrande](https://arxiv.org/abs/1907.10641)                      | A challenging benchmark focusing on commonsense reasoning by resolving pronoun ambiguity.                                       |
| [CQA](https://arxiv.org/abs/1811.00937)                             | Tests conversational question answering where LLMs need to follow the flow of conversational history.                           |
| [OBQA](https://arxiv.org/abs/1809.02789)                            | Evaluates a model's ability to answer open-ended questions requiring factual knowledge retrieval.                               |
| [ARC-e/ARC-c](https://arxiv.org/abs/1911.01547)                     | A set of science exam questions measuring reasoning and understanding. 'e' stands for easy and 'c' for challenging.             |
| [TriviaQA](https://arxiv.org/abs/1705.03551)                        | Assesses LLMs on open-domain trivia questions obtained from real sources.                                                       |
| [NQ](https://github.com/google-research-datasets/natural-questions) | Evaluates question answering on challenging real-world Google search queries.                                                   |
| [HumanEval](https://arxiv.org/abs/2107.03374)                       | Involves direct human judgment of LLM-generated text for coherence, relevance, and other qualities.                             |
| [MBPP](https://arxiv.org/abs/2108.07732)                            | Examines model performance on different mathematical problem-solving subtasks.                                                  |
| [GSM8K](https://arxiv.org/abs/2110.14168)                           | Evaluates LLMs on challenging multi-step grade-school mathematical problems.                                                    |
| [MATH](https://arxiv.org/abs/2103.03874)                            | Another dataset for assessing mathematical reasoning skills in language models.                                                 |
| [AGIEval](https://arxiv.org/abs/2304.06364)                         | Tests an LLM's ability to reason and answer questions based on scenes and images.                                               |
| [BBH](https://arxiv.org/abs/2206.04615)                             | Benchmark for logical, multi-hop reasoning on different types of relations.                                                     |
